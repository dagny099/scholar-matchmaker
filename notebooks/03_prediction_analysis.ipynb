{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link Prediction Analysis and Evaluation\n",
    "\n",
    "This notebook evaluates the trained TransE model using comprehensive metrics and generates high-confidence citation predictions. We implement the evaluation pipeline described in the README:\n",
    "\n",
    "## Evaluation Metrics:\n",
    "- **Mean Reciprocal Rank (MRR)**: Measures ranking quality\n",
    "- **Hits@K**: Proportion of correct predictions in top-K\n",
    "- **AUC & Average Precision**: Binary classification performance\n",
    "\n",
    "## Analysis Pipeline:\n",
    "1. Load trained model and test data\n",
    "2. Comprehensive evaluation with ranking metrics\n",
    "3. Generate missing citation predictions\n",
    "4. Qualitative analysis of predictions\n",
    "5. Performance visualization and reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our modules\n",
    "from src.db import Neo4jConnection\n",
    "from src.data_extraction import load_citation_graph\n",
    "from src.model import TransETrainer\n",
    "from src.evaluation import LinkPredictionEvaluator, evaluate_model\n",
    "from src.visualization import (\n",
    "    plot_evaluation_results,\n",
    "    plot_prediction_analysis,\n",
    "    plot_embedding_visualization,\n",
    "    set_portfolio_style\n",
    ")\n",
    "\n",
    "# Set up environment\n",
    "load_dotenv()\n",
    "set_portfolio_style()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Trained Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model\n",
    "model_path = \"../models/transe_citation_model.pt\"\n",
    "print(f\"Loading trained model from {model_path}...\")\n",
    "\n",
    "try:\n",
    "    trainer = TransETrainer.load_model(model_path, device=device)\n",
    "    print(\"‚úÖ Model loaded successfully\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Model file not found. Please run 02_model_training.ipynb first.\")\n",
    "    raise\n",
    "\n",
    "# Load entity mapping\n",
    "mapping_path = \"../models/entity_mapping.pkl\"\n",
    "with open(mapping_path, 'rb') as f:\n",
    "    entity_mapping = pickle.load(f)\n",
    "\n",
    "# Load training metadata\n",
    "metadata_path = \"../models/training_metadata.pkl\"\n",
    "with open(metadata_path, 'rb') as f:\n",
    "    training_metadata = pickle.load(f)\n",
    "\n",
    "print(f\"\\nüìã Model Configuration:\")\n",
    "print(f\"‚Ä¢ Entities: {training_metadata['num_entities']:,}\")\n",
    "print(f\"‚Ä¢ Embedding dim: {training_metadata['embedding_dim']}\")\n",
    "print(f\"‚Ä¢ Training epochs: {training_metadata['epochs']}\")\n",
    "print(f\"‚Ä¢ Final training loss: {training_metadata['final_loss']:.4f}\")\n",
    "print(f\"‚Ä¢ Model parameters: {sum(p.numel() for p in trainer.model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reload Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconnect to database and reload data for evaluation\n",
    "print(\"Reconnecting to database and preparing test data...\")\n",
    "\n",
    "db = Neo4jConnection()\n",
    "if not db.test_connection():\n",
    "    raise ConnectionError(\"Failed to connect to Neo4j database\")\n",
    "\n",
    "# Reload citation graph (needed for consistent train/test split)\n",
    "extractor = load_citation_graph(db)\n",
    "\n",
    "# Recreate the same train/test split used during training\n",
    "training_data = extractor.create_training_data(\n",
    "    test_size=0.2,\n",
    "    negative_ratio=1,\n",
    "    random_state=42  # Same random state as training\n",
    ")\n",
    "\n",
    "# Extract test data\n",
    "test_edges = training_data['test_edges']\n",
    "test_labels = training_data['test_labels']\n",
    "\n",
    "# Split into positive and negative test samples\n",
    "test_pos_mask = test_labels == 1\n",
    "test_neg_mask = test_labels == 0\n",
    "\n",
    "test_pos_edges = test_edges[test_pos_mask]\n",
    "test_neg_edges = test_edges[test_neg_mask]\n",
    "\n",
    "print(f\"\\nüìä Test Data:\")\n",
    "print(f\"‚Ä¢ Total test samples: {len(test_edges):,}\")\n",
    "print(f\"‚Ä¢ Positive samples: {len(test_pos_edges):,}\")\n",
    "print(f\"‚Ä¢ Negative samples: {len(test_neg_edges):,}\")\n",
    "print(f\"‚Ä¢ Test data ready for evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comprehensive Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive evaluation\n",
    "print(\"Running comprehensive model evaluation...\")\n",
    "print(\"This may take several minutes for ranking metrics.\\n\")\n",
    "\n",
    "# Evaluation parameters\n",
    "K_VALUES = [1, 3, 10]  # For Hits@K evaluation\n",
    "\n",
    "# Run evaluation\n",
    "results = evaluate_model(\n",
    "    trainer=trainer,\n",
    "    test_pos_edges=test_pos_edges,\n",
    "    test_neg_edges=test_neg_edges,\n",
    "    entity_mapping=entity_mapping,\n",
    "    k_values=K_VALUES\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìä EVALUATION RESULTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"\\nüéØ Ranking Metrics:\")\n",
    "print(f\"‚Ä¢ Mean Reciprocal Rank (MRR): {results['mrr']:.4f}\")\n",
    "for k in K_VALUES:\n",
    "    print(f\"‚Ä¢ Hits@{k}: {results['hits'][k]:.3f}\")\n",
    "\n",
    "print(f\"\\nüìà Classification Metrics:\")\n",
    "print(f\"‚Ä¢ AUC Score: {results['auc']:.4f}\")\n",
    "print(f\"‚Ä¢ Average Precision: {results['average_precision']:.4f}\")\n",
    "\n",
    "# Store results for visualization\n",
    "evaluation_results = results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Performance Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpret the results\n",
    "print(\"\\nüîç PERFORMANCE INTERPRETATION:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "mrr = results['mrr']\n",
    "hits_1 = results['hits'][1]\n",
    "hits_10 = results['hits'][10]\n",
    "auc = results['auc']\n",
    "\n",
    "# MRR interpretation\n",
    "if mrr > 0.3:\n",
    "    mrr_quality = \"Excellent\"\n",
    "elif mrr > 0.2:\n",
    "    mrr_quality = \"Good\"\n",
    "elif mrr > 0.1:\n",
    "    mrr_quality = \"Fair\"\n",
    "else:\n",
    "    mrr_quality = \"Needs improvement\"\n",
    "\n",
    "print(f\"\\nüìä Ranking Quality: {mrr_quality}\")\n",
    "print(f\"   ‚Ä¢ MRR of {mrr:.4f} means on average, correct citations appear at rank {1/mrr:.1f}\")\n",
    "print(f\"   ‚Ä¢ {hits_1*100:.1f}% of predictions have the correct citation in rank 1\")\n",
    "print(f\"   ‚Ä¢ {hits_10*100:.1f}% of predictions have the correct citation in top 10\")\n",
    "\n",
    "# AUC interpretation\n",
    "if auc > 0.9:\n",
    "    auc_quality = \"Excellent discrimination\"\n",
    "elif auc > 0.8:\n",
    "    auc_quality = \"Good discrimination\"\n",
    "elif auc > 0.7:\n",
    "    auc_quality = \"Fair discrimination\"\n",
    "else:\n",
    "    auc_quality = \"Poor discrimination\"\n",
    "\n",
    "print(f\"\\nüéØ Binary Classification: {auc_quality}\")\n",
    "print(f\"   ‚Ä¢ AUC of {auc:.4f} indicates {auc_quality.lower()} between citations and non-citations\")\n",
    "print(f\"   ‚Ä¢ Model can distinguish real from fake citations with {auc*100:.1f}% accuracy\")\n",
    "\n",
    "# Overall assessment\n",
    "if mrr > 0.15 and auc > 0.75:\n",
    "    overall = \"‚úÖ Strong performance - suitable for citation recommendation\"\n",
    "elif mrr > 0.1 and auc > 0.65:\n",
    "    overall = \"‚ö†Ô∏è  Moderate performance - may need hyperparameter tuning\"\n",
    "else:\n",
    "    overall = \"‚ùå Weak performance - consider model architecture changes\"\n",
    "\n",
    "print(f\"\\nüèÜ Overall Assessment: {overall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive evaluation visualization\n",
    "print(\"Creating evaluation visualizations...\")\n",
    "\n",
    "fig = plot_evaluation_results(\n",
    "    evaluation_results,\n",
    "    figsize=(14, 10),\n",
    "    save_path=\"../outputs/evaluation_results.png\"\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "print(\"Evaluation results visualization saved to ../outputs/evaluation_results.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Missing Citation Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate high-confidence missing citation predictions\n",
    "print(\"Generating missing citation predictions...\")\n",
    "\n",
    "# Create evaluator for predictions\n",
    "evaluator = LinkPredictionEvaluator(\n",
    "    model=trainer.model,\n",
    "    entity_mapping=entity_mapping,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Get existing citations to exclude from predictions\n",
    "existing_citations = set()\n",
    "for source_idx, target_idx in extractor.citation_edges:\n",
    "    source_paper = extractor.id_to_paper[source_idx]\n",
    "    target_paper = extractor.id_to_paper[target_idx]\n",
    "    existing_citations.add((source_paper, target_paper))\n",
    "\n",
    "print(f\"Excluding {len(existing_citations):,} existing citations\")\n",
    "\n",
    "# Sample source papers for prediction (limit for computational efficiency)\n",
    "all_paper_ids = list(entity_mapping.keys())\n",
    "np.random.seed(42)\n",
    "sample_papers = np.random.choice(all_paper_ids, size=min(50, len(all_paper_ids)), replace=False)\n",
    "\n",
    "print(f\"Generating predictions for {len(sample_papers)} sample papers...\")\n",
    "\n",
    "# Generate predictions\n",
    "predictions_df = evaluator.predict_missing_citations(\n",
    "    source_papers=sample_papers.tolist(),\n",
    "    top_k=20,  # Top 20 predictions per paper\n",
    "    exclude_existing=True,\n",
    "    existing_citations=existing_citations\n",
    ")\n",
    "\n",
    "print(f\"\\nüìã Generated {len(predictions_df):,} citation predictions\")\n",
    "print(f\"‚Ä¢ Average predictions per paper: {len(predictions_df) / len(sample_papers):.1f}\")\n",
    "print(f\"‚Ä¢ Score range: {predictions_df['score'].min():.4f} to {predictions_df['score'].max():.4f}\")\n",
    "print(f\"‚Ä¢ Lower scores indicate higher likelihood\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analyze Top Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine top predictions\n",
    "print(\"\\nüèÜ TOP 20 CITATION PREDICTIONS:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "top_predictions = predictions_df.head(20)\n",
    "\n",
    "for idx, row in top_predictions.iterrows():\n",
    "    source_title = row['source_paper'][:60] + \"...\" if len(row['source_paper']) > 60 else row['source_paper']\n",
    "    target_title = row['target_paper'][:60] + \"...\" if len(row['target_paper']) > 60 else row['target_paper']\n",
    "    score = row['score']\n",
    "    rank = row['rank']\n",
    "    \n",
    "    print(f\"\\n{idx+1:2d}. Score: {score:.4f} | Rank: {rank}\")\n",
    "    print(f\"    Source: {source_title}\")\n",
    "    print(f\"    Target: {target_title}\")\n",
    "    print(f\"    {'-'*75}\")\n",
    "\n",
    "# Save predictions to CSV for further analysis\n",
    "predictions_path = \"../outputs/citation_predictions.csv\"\n",
    "predictions_df.to_csv(predictions_path, index=False)\n",
    "print(f\"\\nüíæ All predictions saved to {predictions_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Prediction Analysis Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize prediction analysis\n",
    "print(\"Creating prediction analysis visualization...\")\n",
    "\n",
    "fig = plot_prediction_analysis(\n",
    "    predictions_df,\n",
    "    top_n=50,\n",
    "    figsize=(16, 12),\n",
    "    save_path=\"../outputs/prediction_analysis.png\"\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "print(\"Prediction analysis saved to ../outputs/prediction_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Embedding Space Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize learned embeddings in 2D\n",
    "print(\"Creating embedding space visualization...\")\n",
    "\n",
    "# Get paper metadata for coloring\n",
    "try:\n",
    "    paper_metadata = extractor.get_paper_metadata()\n",
    "    print(f\"Loaded metadata for {len(paper_metadata)} papers\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load metadata: {e}\")\n",
    "    # Create dummy metadata\n",
    "    paper_metadata = pd.DataFrame({\n",
    "        'paper_id': list(entity_mapping.keys()),\n",
    "        'citations': [0] * len(entity_mapping)\n",
    "    })\n",
    "\n",
    "# Get embeddings\n",
    "with torch.no_grad():\n",
    "    embeddings = trainer.model.entity_embeddings.weight.cpu()\n",
    "\n",
    "# Sample embeddings for visualization (t-SNE is expensive)\n",
    "sample_size = min(1000, len(embeddings))\n",
    "sample_indices = torch.randperm(len(embeddings))[:sample_size]\n",
    "sample_embeddings = embeddings[sample_indices]\n",
    "sample_metadata = paper_metadata.iloc[sample_indices.numpy()] if len(paper_metadata) >= sample_size else paper_metadata\n",
    "\n",
    "print(f\"Visualizing {sample_size} embeddings with t-SNE...\")\n",
    "\n",
    "# Create embedding visualization\n",
    "fig = plot_embedding_visualization(\n",
    "    sample_embeddings,\n",
    "    sample_metadata,\n",
    "    method='tsne',\n",
    "    figsize=(12, 8),\n",
    "    save_path=\"../outputs/embedding_visualization.png\"\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "print(\"Embedding visualization saved to ../outputs/embedding_visualization.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Confidence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze model confidence and prediction quality\n",
    "print(\"Analyzing model confidence and prediction patterns...\")\n",
    "\n",
    "# Score distribution analysis\n",
    "score_stats = predictions_df['score'].describe()\n",
    "print(f\"\\nüìä Prediction Score Statistics:\")\n",
    "print(f\"‚Ä¢ Mean: {score_stats['mean']:.4f}\")\n",
    "print(f\"‚Ä¢ Std: {score_stats['std']:.4f}\")\n",
    "print(f\"‚Ä¢ Min: {score_stats['min']:.4f}\")\n",
    "print(f\"‚Ä¢ Max: {score_stats['max']:.4f}\")\n",
    "\n",
    "# High confidence predictions (lowest scores)\n",
    "high_confidence = predictions_df[predictions_df['score'] <= predictions_df['score'].quantile(0.1)]\n",
    "print(f\"\\nüéØ High Confidence Predictions (top 10%):\")\n",
    "print(f\"‚Ä¢ Count: {len(high_confidence):,}\")\n",
    "print(f\"‚Ä¢ Score threshold: {predictions_df['score'].quantile(0.1):.4f}\")\n",
    "print(f\"‚Ä¢ These represent the most likely missing citations\")\n",
    "\n",
    "# Papers with most predictions\n",
    "source_counts = predictions_df['source_paper'].value_counts().head(10)\n",
    "print(f\"\\nüìù Papers with Most Predictions:\")\n",
    "for paper, count in source_counts.items():\n",
    "    short_title = paper[:80] + \"...\" if len(paper) > 80 else paper\n",
    "    print(f\"‚Ä¢ {count:2d} predictions: {short_title}\")\n",
    "\n",
    "# Papers most frequently predicted as targets\n",
    "target_counts = predictions_df['target_paper'].value_counts().head(10)\n",
    "print(f\"\\nüéØ Most Frequently Predicted Target Papers:\")\n",
    "for paper, count in target_counts.items():\n",
    "    short_title = paper[:80] + \"...\" if len(paper) > 80 else paper\n",
    "    print(f\"‚Ä¢ {count:2d} predictions: {short_title}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Evaluation Summary and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive evaluation summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä COMPREHENSIVE EVALUATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nüéØ Model Performance:\")\n",
    "print(f\"   ‚Ä¢ Mean Reciprocal Rank: {results['mrr']:.4f} ({mrr_quality})\")\n",
    "print(f\"   ‚Ä¢ Hits@1: {results['hits'][1]:.3f} ({results['hits'][1]*100:.1f}% top-1 accuracy)\")\n",
    "print(f\"   ‚Ä¢ Hits@10: {results['hits'][10]:.3f} ({results['hits'][10]*100:.1f}% top-10 accuracy)\")\n",
    "print(f\"   ‚Ä¢ AUC Score: {results['auc']:.4f} ({auc_quality})\")\n",
    "\n",
    "print(f\"\\nüìã Dataset Context:\")\n",
    "print(f\"   ‚Ä¢ Test samples: {len(test_edges):,} ({len(test_pos_edges):,} positive, {len(test_neg_edges):,} negative)\")\n",
    "print(f\"   ‚Ä¢ Entity vocabulary: {training_metadata['num_entities']:,} papers\")\n",
    "print(f\"   ‚Ä¢ Network sparsity: {len(extractor.citation_edges) / (training_metadata['num_entities']**2):.6f}\")\n",
    "\n",
    "print(f\"\\nüîÆ Prediction Generation:\")\n",
    "print(f\"   ‚Ä¢ Generated predictions for {len(sample_papers)} papers\")\n",
    "print(f\"   ‚Ä¢ Total predictions: {len(predictions_df):,}\")\n",
    "print(f\"   ‚Ä¢ High-confidence predictions: {len(high_confidence):,}\")\n",
    "print(f\"   ‚Ä¢ Score range: {predictions_df['score'].min():.4f} to {predictions_df['score'].max():.4f}\")\n",
    "\n",
    "print(f\"\\nüí° Key Insights:\")\n",
    "if results['hits'][1] > 0.1:\n",
    "    print(f\"   ‚Ä¢ Model successfully identifies missing citations with {results['hits'][1]*100:.1f}% top-1 accuracy\")\n",
    "else:\n",
    "    print(f\"   ‚Ä¢ Model struggles with precise ranking (low Hits@1)\")\n",
    "\n",
    "if results['auc'] > 0.8:\n",
    "    print(f\"   ‚Ä¢ Strong discrimination between citations and non-citations\")\n",
    "else:\n",
    "    print(f\"   ‚Ä¢ Moderate ability to distinguish citations from non-citations\")\n",
    "\n",
    "print(f\"   ‚Ä¢ Average predicted rank for true citations: {1/results['mrr']:.1f}\")\n",
    "print(f\"   ‚Ä¢ Model learned meaningful embeddings in {training_metadata['embedding_dim']}D space\")\n",
    "\n",
    "print(f\"\\nüìÅ Generated Files:\")\n",
    "print(f\"   ‚Ä¢ ../outputs/evaluation_results.png - Performance metrics visualization\")\n",
    "print(f\"   ‚Ä¢ ../outputs/prediction_analysis.png - Prediction analysis plots\")\n",
    "print(f\"   ‚Ä¢ ../outputs/embedding_visualization.png - t-SNE embedding visualization\")\n",
    "print(f\"   ‚Ä¢ ../outputs/citation_predictions.csv - All citation predictions\")\n",
    "\n",
    "print(f\"\\nüéì Research Value:\")\n",
    "print(f\"   ‚Ä¢ {len(high_confidence):,} high-confidence missing citations identified\")\n",
    "print(f\"   ‚Ä¢ Demonstrates feasibility of ML-based citation recommendation\")\n",
    "print(f\"   ‚Ä¢ Embeddings capture semantic relationships between papers\")\n",
    "print(f\"   ‚Ä¢ Ready for notebook 04_story_visualization.ipynb\")\n",
    "\n",
    "print(\"\\n‚úÖ Prediction analysis complete! Model evaluation successful.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation results for story visualization\n",
    "eval_summary = {\n",
    "    'metrics': evaluation_results,\n",
    "    'training_metadata': training_metadata,\n",
    "    'prediction_stats': {\n",
    "        'total_predictions': len(predictions_df),\n",
    "        'high_confidence': len(high_confidence),\n",
    "        'sample_papers': len(sample_papers),\n",
    "        'score_range': (predictions_df['score'].min(), predictions_df['score'].max())\n",
    "    },\n",
    "    'interpretation': {\n",
    "        'mrr_quality': mrr_quality,\n",
    "        'auc_quality': auc_quality,\n",
    "        'overall_assessment': overall\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('../outputs/evaluation_summary.pkl', 'wb') as f:\n",
    "    pickle.dump(eval_summary, f)\n",
    "\n",
    "print(\"Evaluation summary saved for story visualization.\")\n",
    "\n",
    "# Clean up\n",
    "db.close()\n",
    "if device.type == 'cuda':\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nCleanup complete. Ready for story visualization!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}