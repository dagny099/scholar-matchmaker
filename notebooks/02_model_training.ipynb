{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TransE Model Training\n",
    "\n",
    "This notebook implements and trains the TransE (Translating Embeddings) model for citation link prediction. The model learns embeddings such that for each citation relationship (source_paper, target_paper), the embedding of source_paper + relation ≈ embedding of target_paper.\n",
    "\n",
    "## Training Pipeline:\n",
    "1. Load and prepare citation data\n",
    "2. Create train/test splits with negative sampling\n",
    "3. Initialize and configure TransE model\n",
    "4. Train with margin ranking loss\n",
    "5. Monitor training progress\n",
    "6. Save trained model for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our modules\n",
    "from src.db import Neo4jConnection\n",
    "from src.data_extraction import load_citation_graph\n",
    "from src.model import create_model, TransE, TransETrainer\n",
    "from src.visualization import plot_training_history, set_portfolio_style\n",
    "\n",
    "# Set up environment\n",
    "load_dotenv()\n",
    "set_portfolio_style()\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Citation Network Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to database and load data\n",
    "print(\"Connecting to Neo4j and loading citation network...\")\n",
    "\n",
    "db = Neo4jConnection()\n",
    "if not db.test_connection():\n",
    "    raise ConnectionError(\"Failed to connect to Neo4j database\")\n",
    "\n",
    "# Load citation graph\n",
    "extractor = load_citation_graph(db)\n",
    "\n",
    "print(f\"\\nDataset loaded:\")\n",
    "print(f\"• Papers: {extractor.num_entities:,}\")\n",
    "print(f\"• Citations: {len(extractor.citation_edges):,}\")\n",
    "print(f\"• Average citations per paper: {len(extractor.citation_edges) / extractor.num_entities:.2f}\")\n",
    "\n",
    "# Get network statistics for context\n",
    "stats = extractor.get_dataset_stats()\n",
    "print(f\"• Network density: {stats['density']:.6f}\")\n",
    "print(f\"• Average degree: {stats['avg_degree']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/test splits with negative sampling\n",
    "print(\"Creating training data with train/test split...\")\n",
    "\n",
    "# Parameters for data preparation\n",
    "TEST_SIZE = 0.2         # 80/20 train/test split\n",
    "NEGATIVE_RATIO = 1      # 1:1 ratio of negative to positive samples\n",
    "RANDOM_STATE = 42       # For reproducible results\n",
    "\n",
    "training_data = extractor.create_training_data(\n",
    "    test_size=TEST_SIZE,\n",
    "    negative_ratio=NEGATIVE_RATIO,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining data prepared:\")\n",
    "print(f\"• Train edges: {len(training_data['train_edges']):,}\")\n",
    "print(f\"• Test edges: {len(training_data['test_edges']):,}\")\n",
    "print(f\"• Train positive/negative ratio: 1:{NEGATIVE_RATIO}\")\n",
    "print(f\"• Entity vocabulary size: {training_data['num_entities']:,}\")\n",
    "\n",
    "# Extract data for training\n",
    "train_edges = training_data['train_edges']\n",
    "train_labels = training_data['train_labels']\n",
    "test_edges = training_data['test_edges']\n",
    "test_labels = training_data['test_labels']\n",
    "num_entities = training_data['num_entities']\n",
    "entity_mapping = training_data['entity_mapping']\n",
    "\n",
    "# Split positive and negative samples for training\n",
    "train_pos_mask = train_labels == 1\n",
    "train_neg_mask = train_labels == 0\n",
    "\n",
    "train_pos_edges = train_edges[train_pos_mask]\n",
    "train_neg_edges = train_edges[train_neg_mask]\n",
    "\n",
    "print(f\"\\nSplit breakdown:\")\n",
    "print(f\"• Training positive: {len(train_pos_edges):,}\")\n",
    "print(f\"• Training negative: {len(train_neg_edges):,}\")\n",
    "print(f\"• Test samples: {len(test_edges):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Configuration and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "EMBEDDING_DIM = 128     # Dimension of entity embeddings\n",
    "MARGIN = 1.0           # Margin for ranking loss\n",
    "LEARNING_RATE = 0.01   # Learning rate for Adam optimizer\n",
    "P_NORM = 1             # L1 norm for TransE scoring (can be 1 or 2)\n",
    "\n",
    "print(\"Model Configuration:\")\n",
    "print(f\"• Embedding dimension: {EMBEDDING_DIM}\")\n",
    "print(f\"• Margin: {MARGIN}\")\n",
    "print(f\"• Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"• P-norm: {P_NORM}\")\n",
    "print(f\"• Device: {device}\")\n",
    "\n",
    "# Create TransE model\n",
    "print(\"\\nInitializing TransE model...\")\n",
    "trainer = create_model(\n",
    "    num_entities=num_entities,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    margin=MARGIN,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"Model created with {num_entities:,} entities\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in trainer.model.parameters()):,}\")\n",
    "\n",
    "# Display model architecture\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(trainer.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "EPOCHS = 100           # Number of training epochs\n",
    "BATCH_SIZE = 1024      # Batch size for training\n",
    "VERBOSE = True         # Show training progress\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"• Epochs: {EPOCHS}\")\n",
    "print(f\"• Batch size: {BATCH_SIZE}\")\n",
    "print(f\"• Total training samples: {len(train_pos_edges):,}\")\n",
    "print(f\"• Batches per epoch: {(len(train_pos_edges) + BATCH_SIZE - 1) // BATCH_SIZE}\")\n",
    "print(f\"• Estimated training time: ~{EPOCHS * len(train_pos_edges) // (BATCH_SIZE * 1000)} minutes\")\n",
    "\n",
    "# Move training data to device\n",
    "print(\"\\nMoving data to device...\")\n",
    "train_pos_edges = train_pos_edges.to(device)\n",
    "train_neg_edges = train_neg_edges.to(device)\n",
    "\n",
    "print(\"Ready to start training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Starting TransE model training...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "training_history = trainer.train(\n",
    "    positive_edges=train_pos_edges,\n",
    "    negative_edges=train_neg_edges,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=VERBOSE\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Training completed!\")\n",
    "print(f\"Final training loss: {training_history['loss'][-1]:.4f}\")\n",
    "print(f\"Loss reduction: {training_history['loss'][0]:.4f} → {training_history['loss'][-1]:.4f}\")\n",
    "print(f\"Improvement: {((training_history['loss'][0] - training_history['loss'][-1]) / training_history['loss'][0] * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Progress Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss history\n",
    "print(\"Visualizing training progress...\")\n",
    "\n",
    "fig = plot_training_history(\n",
    "    training_history['loss'],\n",
    "    figsize=(12, 6),\n",
    "    save_path=\"../outputs/training_history.png\"\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# Additional training statistics\n",
    "print(\"\\n📊 Training Statistics:\")\n",
    "print(f\"• Total epochs: {len(training_history['loss'])}\")\n",
    "print(f\"• Initial loss: {training_history['loss'][0]:.4f}\")\n",
    "print(f\"• Final loss: {training_history['loss'][-1]:.4f}\")\n",
    "print(f\"• Best loss: {min(training_history['loss']):.4f}\")\n",
    "print(f\"• Loss variance: {np.var(training_history['loss'][-10:]):.6f} (last 10 epochs)\")\n",
    "\n",
    "# Check convergence\n",
    "recent_losses = training_history['loss'][-20:] if len(training_history['loss']) >= 20 else training_history['loss']\n",
    "loss_std = np.std(recent_losses)\n",
    "if loss_std < 0.01:\n",
    "    print(\"✅ Model appears to have converged (stable recent loss)\")\n",
    "else:\n",
    "    print(\"⚠️  Model may benefit from additional training epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect learned embeddings\n",
    "print(\"Inspecting learned embeddings...\")\n",
    "\n",
    "# Get entity embeddings\n",
    "with torch.no_grad():\n",
    "    entity_embeddings = trainer.model.entity_embeddings.weight.cpu()\n",
    "    relation_embedding = trainer.model.relation_embedding.weight.cpu()\n",
    "\n",
    "print(f\"\\n📐 Embedding Properties:\")\n",
    "print(f\"• Entity embeddings shape: {entity_embeddings.shape}\")\n",
    "print(f\"• Relation embedding shape: {relation_embedding.shape}\")\n",
    "print(f\"• Entity embedding norm (mean): {torch.norm(entity_embeddings, dim=1).mean():.4f}\")\n",
    "print(f\"• Entity embedding norm (std): {torch.norm(entity_embeddings, dim=1).std():.4f}\")\n",
    "print(f\"• Relation embedding norm: {torch.norm(relation_embedding).item():.4f}\")\n",
    "\n",
    "# Analyze embedding distribution\n",
    "embedding_stats = {\n",
    "    'mean': entity_embeddings.mean().item(),\n",
    "    'std': entity_embeddings.std().item(),\n",
    "    'min': entity_embeddings.min().item(),\n",
    "    'max': entity_embeddings.max().item()\n",
    "}\n",
    "\n",
    "print(f\"\\n📈 Embedding Value Distribution:\")\n",
    "for stat, value in embedding_stats.items():\n",
    "    print(f\"• {stat.capitalize()}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Quick Training Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick validation on training data\n",
    "print(\"Performing quick training validation...\")\n",
    "\n",
    "trainer.model.eval()\n",
    "with torch.no_grad():\n",
    "    # Sample some training examples\n",
    "    sample_size = min(1000, len(train_pos_edges))\n",
    "    sample_indices = torch.randperm(len(train_pos_edges))[:sample_size]\n",
    "    \n",
    "    sample_pos = train_pos_edges[sample_indices]\n",
    "    sample_neg = train_neg_edges[sample_indices]\n",
    "    \n",
    "    # Get scores (lower = more likely)\n",
    "    pos_scores = trainer.model(sample_pos[:, 0], sample_pos[:, 1])\n",
    "    neg_scores = trainer.model(sample_neg[:, 0], sample_neg[:, 1])\n",
    "    \n",
    "    # Calculate metrics\n",
    "    pos_mean = pos_scores.mean().item()\n",
    "    neg_mean = neg_scores.mean().item()\n",
    "    \n",
    "    print(f\"\\n🎯 Training Sample Validation ({sample_size} samples):\")\n",
    "    print(f\"• Average positive score: {pos_mean:.4f}\")\n",
    "    print(f\"• Average negative score: {neg_mean:.4f}\")\n",
    "    print(f\"• Score separation: {neg_mean - pos_mean:.4f}\")\n",
    "    \n",
    "    if neg_mean > pos_mean:\n",
    "        print(\"✅ Model correctly assigns lower scores to positive samples\")\n",
    "        separation_quality = \"Good\" if (neg_mean - pos_mean) > 0.5 else \"Moderate\"\n",
    "        print(f\"• Separation quality: {separation_quality}\")\n",
    "    else:\n",
    "        print(\"⚠️  Model may need more training - negative samples have lower scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model_path = \"../models/transe_citation_model.pt\"\n",
    "print(f\"Saving trained model to {model_path}...\")\n",
    "\n",
    "trainer.save_model(model_path)\n",
    "\n",
    "# Also save entity mapping for later use\n",
    "import pickle\n",
    "mapping_path = \"../models/entity_mapping.pkl\"\n",
    "with open(mapping_path, 'wb') as f:\n",
    "    pickle.dump(entity_mapping, f)\n",
    "\n",
    "# Save training metadata\n",
    "metadata = {\n",
    "    'num_entities': num_entities,\n",
    "    'num_train_edges': len(train_edges),\n",
    "    'num_test_edges': len(test_edges),\n",
    "    'embedding_dim': EMBEDDING_DIM,\n",
    "    'margin': MARGIN,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'epochs': EPOCHS,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'final_loss': training_history['loss'][-1],\n",
    "    'device': str(device)\n",
    "}\n",
    "\n",
    "metadata_path = \"../models/training_metadata.pkl\"\n",
    "with open(metadata_path, 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "\n",
    "print(f\"\\n💾 Files saved:\")\n",
    "print(f\"• Model: {model_path}\")\n",
    "print(f\"• Entity mapping: {mapping_path}\")\n",
    "print(f\"• Training metadata: {metadata_path}\")\n",
    "print(f\"• Training plot: ../outputs/training_history.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive training summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🎓 TRAINING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n📊 Dataset:\")\n",
    "print(f\"   • {num_entities:,} papers (entities)\")\n",
    "print(f\"   • {len(extractor.citation_edges):,} citation relationships\")\n",
    "print(f\"   • {len(train_edges):,} training examples ({len(train_pos_edges):,} pos, {len(train_neg_edges):,} neg)\")\n",
    "print(f\"   • {len(test_edges):,} test examples\")\n",
    "\n",
    "print(f\"\\n🧠 Model Architecture:\")\n",
    "print(f\"   • TransE with {EMBEDDING_DIM}-dimensional embeddings\")\n",
    "print(f\"   • {sum(p.numel() for p in trainer.model.parameters()):,} total parameters\")\n",
    "print(f\"   • Margin ranking loss with margin = {MARGIN}\")\n",
    "print(f\"   • L{P_NORM} norm for distance computation\")\n",
    "\n",
    "print(f\"\\n⚙️  Training Configuration:\")\n",
    "print(f\"   • {EPOCHS} epochs with batch size {BATCH_SIZE}\")\n",
    "print(f\"   • Adam optimizer with learning rate {LEARNING_RATE}\")\n",
    "print(f\"   • Trained on {device}\")\n",
    "\n",
    "print(f\"\\n📈 Results:\")\n",
    "print(f\"   • Final loss: {training_history['loss'][-1]:.4f}\")\n",
    "print(f\"   • Loss improvement: {((training_history['loss'][0] - training_history['loss'][-1]) / training_history['loss'][0] * 100):.1f}%\")\n",
    "print(f\"   • Training converged: {'Yes' if loss_std < 0.01 else 'Potentially needs more epochs'}\")\n",
    "\n",
    "print(f\"\\n🎯 Next Steps:\")\n",
    "print(f\"   • Run notebook 03_prediction_analysis.ipynb for comprehensive evaluation\")\n",
    "print(f\"   • Evaluate with MRR, Hits@K, and AUC metrics\")\n",
    "print(f\"   • Generate missing citation predictions\")\n",
    "print(f\"   • Create story visualizations\")\n",
    "\n",
    "print(\"\\n✅ Model training complete! Ready for evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "db.close()\n",
    "print(\"Database connection closed.\")\n",
    "\n",
    "# Clear GPU memory if using CUDA\n",
    "if device.type == 'cuda':\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"GPU memory cleared.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",\n",
   "language": "python",\n   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}